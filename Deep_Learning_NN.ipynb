{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Deep Learning_NN.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK4tC51-RnkI"
      },
      "source": [
        "",
        "\n",
        "",
        "",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alZdbcBVRnkO"
      },
      "source": [
        "## Part 1:  Text Classification\n",
        "\n",
        "General comments and any shared processing here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1bB4IPMRnkS"
      },
      "source": [
        "### Part 1: Neural Network Method\n",
        "\n",
        "Details of method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU3EiXdwRnkT"
      },
      "source": [
        "import keras\n",
        "import numpy as np                                                  # import all necessary libraries\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "#!pip install pandas\n",
        "import pandas as pd\n",
        "from keras import layers\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIUGSsxLRnkV"
      },
      "source": [
        "df_train = pd.read_csv('axcs_train.csv')\n",
        "\n",
        "df_test = pd.read_csv('axcs_test.csv')\n",
        "\n",
        "d_x_train = df_train['Abstract'].values              # Load datasets\n",
        "maxlen = 100                                       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NsTKsDIRnkW"
      },
      "source": [
        "df_test['Abstract'] = df_test.Abstract.astype(str)\n",
        "d_x_test=df_test['Abstract'].values\n",
        "tokenizer = Tokenizer(num_words=1000)                      # tokenization process\n",
        "tokenizer.fit_on_texts(d_x_train)\n",
        "tokenizer.fit_on_texts(d_x_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5ABSN5YRnkY"
      },
      "source": [
        "x_train_arr = tokenizer.texts_to_sequences(d_x_train)\n",
        "x_test_arr = tokenizer.texts_to_sequences(d_x_test)\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_QR6UaORnkZ"
      },
      "source": [
        "X_train= np.array(x_train_arr)             # form an np array to store integer values \n",
        "X_test= np.array(x_test_arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2DHshhlRnka",
        "outputId": "fdb11c74-5b52-45b0-a790-2dd6870452b5"
      },
      "source": [
        "X_train = pad_sequences(X_train,padding='post', maxlen=maxlen)         # pad sequences for a fed length of input \n",
        "X_test  = pad_sequences(X_test,padding='post', maxlen=maxlen) \n",
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  3, 506,  91, ...,   0,   0,   0],\n",
              "       [  3,  12,   9, ...,   0,   0,   0],\n",
              "       [462,   2,   9, ...,   0,   0,   0],\n",
              "       ...,\n",
              "       [ 59,   8,   1, ...,   0,   0,   0],\n",
              "       [738,   6, 433, ...,   2,   4,  14],\n",
              "       [827, 507, 625, ...,   7,  42,  46]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNe6fLL-Rnkc",
        "outputId": "20af1233-04fe-44b6-df61-2d911b16be87"
      },
      "source": [
        "embedding_dim = 50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(input_dim=vocab_size, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=maxlen))                      # ADD EMBEDDIG LAYER WITHI THE MODEL SEQUENTIALLY \n",
        "model.add(layers.LSTM(50))                           # adding lstm layer with 50 neurons \n",
        "model.add(layers.Dense(1, activation='sigmoid'))              # Final output sgmoid layer \n",
        "model.compile(optimizer='adam', \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy']) \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 100, 50)           4735050   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 50)                20200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 4,755,301\n",
            "Trainable params: 4,755,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rlo6593nRnke",
        "outputId": "147e5c05-67b2-470f-a16d-6ec309834fbc"
      },
      "source": [
        "y_train=df_train['InfoTheory'].values\n",
        "y_test = df_test['InfoTheory'].values\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=1,                                  # for class infothoery train and fit model\n",
        "                    verbose=True,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False) \n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\GHOST\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 54731 samples, validate on 19679 samples\n",
            "Epoch 1/1\n",
            "54731/54731 [==============================] - 564s 10ms/step - loss: 0.2627 - accuracy: 0.9060 - val_loss: nan - val_accuracy: 0.8214\n",
            "Training Accuracy: 0.8139\n",
            "Testing Accuracy:  0.8214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkzF4ZUNRnkf",
        "outputId": "2c327d08-9e1e-46ae-a34c-6b52292ce4d4"
      },
      "source": [
        "y_train=df_train['CompVis'].values\n",
        "y_test = df_test['CompVis'].values\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=1,\n",
        "                    verbose=True,                               # for class Comp vis\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False) \n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 54731 samples, validate on 19679 samples\n",
            "Epoch 1/1\n",
            "54731/54731 [==============================] - 529s 10ms/step - loss: 0.1214 - accuracy: 0.9658 - val_loss: nan - val_accuracy: 0.9548\n",
            "Training Accuracy: 0.9754\n",
            "Testing Accuracy:  0.9548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDEuAkv6Rnkg",
        "outputId": "18c24bbb-2da6-41c7-b364-8d5a2e5e0fab"
      },
      "source": [
        "y_train=df_train['Math'].values\n",
        "y_test = df_test['Math'].values\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=1,                                     # for class math\n",
        "                    verbose=True,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    batch_size=10)\n",
        "loss, accuracy = model.evaluate(X_train, y_train, verbose=False) \n",
        "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
        "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 54731 samples, validate on 19679 samples\n",
            "Epoch 1/1\n",
            "54731/54731 [==============================] - 514s 9ms/step - loss: 0.3623 - accuracy: 0.8442 - val_loss: nan - val_accuracy: 0.8478\n",
            "Training Accuracy: 0.8668\n",
            "Testing Accuracy:  0.8478\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
